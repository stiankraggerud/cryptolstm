{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow[and-cuda]\n",
    "# %pip install nbformat\n",
    "# %pip install dash\n",
    "# % pip install matplotlib\n",
    "# % pip install plotly\n",
    "# %pip install imbalanced-learn\n",
    "# %pip install scikit-learn\n",
    "# %pip install tqdm\n",
    "# %pip install pandas\n",
    "# %pip install python-binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 21:49:33.727471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-19 21:49:33.730099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-19 21:49:33.730185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 1, 500)            1024000   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 500)               2002000   \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 50)                25050     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3051101 (11.64 MB)\n",
      "Trainable params: 3051101 (11.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      " 1626/30531 [>.............................] - ETA: 51s - loss: 0.8070 - root_mean_squared_error: 0.8722"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(filepath\u001b[39m=\u001b[39mfilename_format, save_freq\u001b[39m=\u001b[39msave_every_n_epochs \u001b[39m*\u001b[39m (\u001b[39mlen\u001b[39m(y_train) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m8\u001b[39m))  \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39m# Trene modellen med ModelCheckpoint som en callback  \u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[checkpoint])  \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=170'>171</a>\u001b[0m \u001b[39m# Lagre modellen\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39m# modelName = stockDataHandler.get_full_path('lstm_model_2018-06_2023_train_v2.h5')\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39m# model.save(modelName)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=174'>175</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m#############################\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import stockDataHandler\n",
    "\n",
    "\n",
    "\n",
    "def rolling_window_normalization(data, column_name, window_size):\n",
    "    \"\"\"\n",
    "    Rullende vindu normalisering.\n",
    "    \"\"\"\n",
    "    rolling_mean = data[column_name].rolling(window=window_size).mean()\n",
    "    rolling_std = data[column_name].rolling(window=window_size).std()\n",
    "    \n",
    "    normalized_column = (data[column_name] - rolling_mean) / rolling_std\n",
    "    return normalized_column\n",
    "\n",
    "def min_max_normalization(data, column_name):\n",
    "    \"\"\"\n",
    "    Min-Max normalisering.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_column = scaler.fit_transform(data[[column_name]])\n",
    "    return normalized_column\n",
    "\n",
    "#filename = 'btc_august2020_november2023_5min.csv'\n",
    "filename = 'btc_2017_november2023_5min.csv'\n",
    "\n",
    "# Les inn data\n",
    "\n",
    "stock_data = stockDataHandler.LoadDataCurrentDirectory(filename)\n",
    "\n",
    "stockDataHandler.SetEMA(stock_data, 50, 'EMA50')\n",
    "stockDataHandler.SetEMA(stock_data, 100, 'EMA100')\n",
    "stockDataHandler.SetMacd(stock_data, 50)\n",
    "\n",
    "window_size = 20  # Valgfri vindustørrelse\n",
    "stock_data['Close_normalized'] = rolling_window_normalization(stock_data, 'Original_Close', window_size)\n",
    "stock_data['Open_normalized'] = rolling_window_normalization(stock_data, 'Original_Open', window_size)\n",
    "stock_data['High_normalized'] = rolling_window_normalization(stock_data, 'Original_High', window_size)\n",
    "stock_data['Low_normalized'] = rolling_window_normalization(stock_data, 'Original_Low', window_size)\n",
    "\n",
    "stock_data['Original_Taker_buy__base_asset_volume_normalized'] = rolling_window_normalization(stock_data, 'Original_Taker_buy__base_asset_volume', window_size)\n",
    "stock_data['Original_Taker_buy__quote_asset_volume_normalized'] = rolling_window_normalization(stock_data, 'Original_Taker_buy__quote_asset_volume', window_size)\n",
    "\n",
    "stock_data['Volume_normalized'] = rolling_window_normalization(stock_data, 'Volume', window_size)\n",
    "stock_data['Number_of_trades_normalized'] = rolling_window_normalization(stock_data, 'Original_Number_of_trades', window_size) \n",
    "\n",
    "# stock_data['Close_normalized'] = min_max_normalization(stock_data, 'Original_Close')\n",
    "# stock_data['Volume_normalized'] = min_max_normalization(stock_data, 'Volume')\n",
    "\n",
    "# Min-Max normalisering\n",
    "stock_data['EMA50_normalized'] = min_max_normalization(stock_data, 'EMA50')\n",
    "stock_data['EMA100_normalized'] = min_max_normalization(stock_data, 'EMA100')\n",
    "stock_data['MACD_normalized'] = min_max_normalization(stock_data, 'MACD')\n",
    "\n",
    "# Fjern NaN-verdier som kan oppstå etter rullende vindu normalisering\n",
    "stock_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "stockDataHandler.CleanData(stock_data)\n",
    "\n",
    "\n",
    "\n",
    "def generate_target(df, column_name, steps_ahead=1):\n",
    "    \"\"\"\n",
    "    Genererer en 'Target'-kolonne basert på fremtidig pris.\n",
    "    Prisen 'steps_ahead' punkter frem i tid vil være målverdien.\n",
    "    \"\"\"\n",
    "    df['Target'] = df[column_name].shift(-steps_ahead)\n",
    "    df.dropna(inplace=True)  # Fjerner NaN-verdier som kan oppstå på grunn av tidsforskyvningen\n",
    "    return df\n",
    "\n",
    "# Bruk funksjonen for å generere 'Target'-kolonnen basert på 'Original_Close'-kolonnen\n",
    "stock_data = generate_target(stock_data, 'Close_normalized',1)\n",
    "\n",
    "def create_sequences(X, y, sequence_length):  \n",
    "    X_sequences = []  \n",
    "    y_sequences = []  \n",
    "  \n",
    "    for i in range(len(X) - sequence_length):  \n",
    "        X_sequences.append(X[i:i + sequence_length])  \n",
    "        y_sequences.append(y[i + sequence_length])  \n",
    "  \n",
    "    X_sequences = np.array(X_sequences)  \n",
    "    y_sequences = np.array(y_sequences)  \n",
    "  \n",
    "    return X_sequences, y_sequences  \n",
    "\n",
    "# Prepare data for sequences  \n",
    "X = stock_data[['Close_normalized','Open_normalized','High_normalized', 'Low_normalized', 'Number_of_trades_normalized', 'Volume_normalized','Original_Taker_buy__base_asset_volume_normalized','Original_Taker_buy__quote_asset_volume_normalized', 'EMA50_normalized', 'EMA100_normalized', 'MACD_normalized']]  \n",
    "y = stock_data['Target']  \n",
    "  \n",
    "sequence_length = 1  # Antall tidssteg du ønsker å bruke som input  \n",
    "  \n",
    "# Create sequences before reshaping and splitting  \n",
    "X_sequences, y_sequences = stockDataHandler.create_sequences(X.values, y.values, sequence_length)  \n",
    "  \n",
    "# Forme data for LSTM (samples, timesteps, features)  \n",
    "X_sequences = np.reshape(X_sequences, (X_sequences.shape[0], sequence_length, X_sequences.shape[2]))  \n",
    "  \n",
    "# Splitte data i trening og testsett  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.3, random_state=42)  \n",
    "\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(500, return_sequences=True, input_shape=(X_train.shape[1], 11))) \n",
    "model.add(layers.LSTM(500, return_sequences=False))\n",
    "model.add(layers.Dense(50))\n",
    "model.add(layers.Dense(1))\n",
    "# model.add(layers.Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):  \n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])  # endre til loss='categorical_crossentropy' for klassifisering\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[root_mean_squared_error])  # endre til loss='categorical_crossentropy' for klassifisering\n",
    "\n",
    "\n",
    "# Trene modellen\n",
    "#model.fit(X_train, y_train, epochs=1000, batch_size=8)\n",
    "\n",
    "# Definer antall epoker for lagring av modellen  \n",
    "save_every_n_epochs = 1  # Endre dette tallet etter ønske  \n",
    "  \n",
    "# Opprett en katalog for å lagre modellene  \n",
    "models_directory = 'saved_models_train_v4'  \n",
    "if not os.path.exists(models_directory):  \n",
    "    os.makedirs(models_directory)  \n",
    "  \n",
    "# # Definer filnavnformatet for å reflektere antall epoker modellen er trent på  \n",
    "\n",
    "  \n",
    "# # Bruk ModelCheckpoint for å lagre modellen for hver n'te epoke  \n",
    "\n",
    "  \n",
    "# # Trene modellen med ModelCheckpoint som en callback  \n",
    "\n",
    "\n",
    "\n",
    "# Definer filnavnformatet for å reflektere antall epoker modellen er trent på  \n",
    "filename_format = os.path.join(models_directory, 'lstm_model_2017-06_2023_train_v4_epoch-{epoch:03d}_500_500_50_1.h5')  \n",
    "\n",
    "# Bruk ModelCheckpoint for å lagre modellen for hver n'te epoke  \n",
    "checkpoint = ModelCheckpoint(filepath=filename_format, save_freq=save_every_n_epochs * (len(y_train) // 8))  \n",
    "\n",
    "# Trene modellen med ModelCheckpoint som en callback  \n",
    "model.fit(X_train, y_train, epochs=30, batch_size=15, callbacks=[checkpoint])  \n",
    "\n",
    "# Lagre modellen\n",
    "# modelName = stockDataHandler.get_full_path('lstm_model_2018-06_2023_train_v2.h5')\n",
    "# model.save(modelName)\n",
    "\n",
    "print('#############################')\n",
    "# Vurdere modellen på testdata\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f'R2 Score: {r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class R2ScoreCallback(Callback):\n",
    "    def __init__(self, x_test, y_test, save_every_n_epochs):\n",
    "        super().__init__()\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.save_every_n_epochs = save_every_n_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_every_n_epochs == 0:\n",
    "            predictions = self.model.predict(self.x_test)\n",
    "            r2 = r2_score(self.y_test, predictions)\n",
    "            print(f'Epoch {epoch + 1}: R2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 1, 500)            1024000   \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 500)               2002000   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                25050     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3051101 (11.64 MB)\n",
      "Trainable params: 3051101 (11.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "6134/6134 [==============================] - 4s 684us/step\n",
      "Epoch 1: R2 Score: 0.5209791110685491\n",
      "30531/30531 [==============================] - 59s 2ms/step - loss: 0.7745 - root_mean_squared_error: 0.8550\n",
      "Epoch 2/30\n",
      "16860/30531 [===============>..............] - ETA: 23s - loss: 0.7699 - root_mean_squared_error: 0.8519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 669us/step\n",
      "Epoch 2: R2 Score: 0.5231380591485574\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7696 - root_mean_squared_error: 0.8518\n",
      "Epoch 3/30\n",
      "6134/6134 [==============================] - 4s 678us/step\n",
      "Epoch 3: R2 Score: 0.5224530026279866\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7688 - root_mean_squared_error: 0.8516\n",
      "Epoch 4/30\n",
      "13052/30531 [===========>..................] - ETA: 30s - loss: 0.7728 - root_mean_squared_error: 0.8541"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 680us/step\n",
      "Epoch 4: R2 Score: 0.5213790879582907\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7683 - root_mean_squared_error: 0.8514\n",
      "Epoch 5/30\n",
      "6134/6134 [==============================] - 4s 675us/step\n",
      "Epoch 5: R2 Score: 0.5228176314523746\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7679 - root_mean_squared_error: 0.8509\n",
      "Epoch 6/30\n",
      " 9221/30531 [========>.....................] - ETA: 36s - loss: 0.7674 - root_mean_squared_error: 0.8503"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 683us/step\n",
      "Epoch 6: R2 Score: 0.5229614603675607\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7675 - root_mean_squared_error: 0.8507\n",
      "Epoch 7/30\n",
      "6134/6134 [==============================] - 4s 679us/step\n",
      "Epoch 7: R2 Score: 0.5226765574060173\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7671 - root_mean_squared_error: 0.8505\n",
      "Epoch 8/30\n",
      " 5418/30531 [====>.........................] - ETA: 42s - loss: 0.7661 - root_mean_squared_error: 0.8501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 683us/step\n",
      "Epoch 8: R2 Score: 0.5231607304215922\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7669 - root_mean_squared_error: 0.8503\n",
      "Epoch 9/30\n",
      "6134/6134 [==============================] - 4s 682us/step\n",
      "Epoch 9: R2 Score: 0.5226320762855394\n",
      "30531/30531 [==============================] - 56s 2ms/step - loss: 0.7667 - root_mean_squared_error: 0.8502\n",
      "Epoch 10/30\n",
      " 1588/30531 [>.............................] - ETA: 50s - loss: 0.7592 - root_mean_squared_error: 0.8456"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 680us/step\n",
      "Epoch 10: R2 Score: 0.5232117732636867\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7665 - root_mean_squared_error: 0.8500\n",
      "Epoch 11/30\n",
      "28302/30531 [==========================>...] - ETA: 3s - loss: 0.7671 - root_mean_squared_error: 0.8506"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 681us/step\n",
      "Epoch 11: R2 Score: 0.5235437566543961\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7664 - root_mean_squared_error: 0.8501\n",
      "Epoch 12/30\n",
      "6134/6134 [==============================] - 4s 683us/step\n",
      "Epoch 12: R2 Score: 0.523646247206973\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7662 - root_mean_squared_error: 0.8501\n",
      "Epoch 13/30\n",
      "24488/30531 [=======================>......] - ETA: 10s - loss: 0.7682 - root_mean_squared_error: 0.8512"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 683us/step\n",
      "Epoch 13: R2 Score: 0.5237910659789158\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7661 - root_mean_squared_error: 0.8501\n",
      "Epoch 14/30\n",
      "6134/6134 [==============================] - 4s 685us/step\n",
      "Epoch 14: R2 Score: 0.5229110153211951\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7661 - root_mean_squared_error: 0.8502\n",
      "Epoch 15/30\n",
      "20670/30531 [===================>..........] - ETA: 16s - loss: 0.7660 - root_mean_squared_error: 0.8500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 704us/step\n",
      "Epoch 15: R2 Score: 0.5234045322278422\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7660 - root_mean_squared_error: 0.8499\n",
      "Epoch 16/30\n",
      "6134/6134 [==============================] - 4s 693us/step\n",
      "Epoch 16: R2 Score: 0.5234700177851543\n",
      "30531/30531 [==============================] - 58s 2ms/step - loss: 0.7657 - root_mean_squared_error: 0.8495\n",
      "Epoch 17/30\n",
      "16850/30531 [===============>..............] - ETA: 23s - loss: 0.7647 - root_mean_squared_error: 0.8493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 681us/step\n",
      "Epoch 17: R2 Score: 0.5232813739147675\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7657 - root_mean_squared_error: 0.8497\n",
      "Epoch 18/30\n",
      "6134/6134 [==============================] - 4s 671us/step\n",
      "Epoch 18: R2 Score: 0.5234275265662977\n",
      "30531/30531 [==============================] - 58s 2ms/step - loss: 0.7656 - root_mean_squared_error: 0.8498\n",
      "Epoch 19/30\n",
      "13039/30531 [===========>..................] - ETA: 30s - loss: 0.7632 - root_mean_squared_error: 0.8482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 669us/step\n",
      "Epoch 19: R2 Score: 0.523261688417757\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7655 - root_mean_squared_error: 0.8495\n",
      "Epoch 20/30\n",
      "6134/6134 [==============================] - 4s 682us/step\n",
      "Epoch 20: R2 Score: 0.5231388374327631\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7653 - root_mean_squared_error: 0.8497\n",
      "Epoch 21/30\n",
      " 9225/30531 [========>.....................] - ETA: 36s - loss: 0.7691 - root_mean_squared_error: 0.8510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 686us/step\n",
      "Epoch 21: R2 Score: 0.5230218134555817\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7652 - root_mean_squared_error: 0.8494\n",
      "Epoch 22/30\n",
      "6134/6134 [==============================] - 4s 692us/step\n",
      "Epoch 22: R2 Score: 0.523754937985941\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7651 - root_mean_squared_error: 0.8494\n",
      "Epoch 23/30\n",
      " 5398/30531 [====>.........................] - ETA: 43s - loss: 0.7586 - root_mean_squared_error: 0.8461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 670us/step\n",
      "Epoch 23: R2 Score: 0.5234349866763549\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7651 - root_mean_squared_error: 0.8494\n",
      "Epoch 24/30\n",
      "6134/6134 [==============================] - 4s 686us/step\n",
      "Epoch 24: R2 Score: 0.5236809416666024\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7650 - root_mean_squared_error: 0.8493\n",
      "Epoch 25/30\n",
      " 1591/30531 [>.............................] - ETA: 49s - loss: 0.7654 - root_mean_squared_error: 0.8497"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flyt/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134/6134 [==============================] - 4s 712us/step\n",
      "Epoch 25: R2 Score: 0.5235119493705538\n",
      "30531/30531 [==============================] - 57s 2ms/step - loss: 0.7648 - root_mean_squared_error: 0.8492\n",
      "Epoch 26/30\n",
      " 4515/30531 [===>..........................] - ETA: 45s - loss: 0.7637 - root_mean_squared_error: 0.8484"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m r2_callback \u001b[39m=\u001b[39m R2ScoreCallback(X_test, y_test, save_every_n_epochs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39m# Legge til både checkpoint og R2ScoreCallback i callbacks-listen\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.250.135.34/home/flyt/dev/cryptolstm/Trial01/lstm_training_five_min_price.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[checkpoint, r2_callback])\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import stockDataHandler\n",
    "\n",
    "\n",
    "\n",
    "def rolling_window_normalization(data, column_name, window_size):\n",
    "    \"\"\"\n",
    "    Rullende vindu normalisering.\n",
    "    \"\"\"\n",
    "    rolling_mean = data[column_name].rolling(window=window_size).mean()\n",
    "    rolling_std = data[column_name].rolling(window=window_size).std()\n",
    "    \n",
    "    normalized_column = (data[column_name] - rolling_mean) / rolling_std\n",
    "    return normalized_column\n",
    "\n",
    "def min_max_normalization(data, column_name):\n",
    "    \"\"\"\n",
    "    Min-Max normalisering.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_column = scaler.fit_transform(data[[column_name]])\n",
    "    return normalized_column\n",
    "\n",
    "#filename = 'btc_august2020_november2023_5min.csv'\n",
    "filename = 'btc_2017_november2023_5min.csv'\n",
    "\n",
    "# Les inn data\n",
    "\n",
    "stock_data = stockDataHandler.LoadDataCurrentDirectory(filename)\n",
    "\n",
    "stockDataHandler.SetEMA(stock_data, 50, 'EMA50')\n",
    "stockDataHandler.SetEMA(stock_data, 100, 'EMA100')\n",
    "stockDataHandler.SetMacd(stock_data, 50)\n",
    "\n",
    "window_size = 20  # Valgfri vindustørrelse\n",
    "stock_data['Close_normalized'] = rolling_window_normalization(stock_data, 'Original_Close', window_size)\n",
    "stock_data['Open_normalized'] = rolling_window_normalization(stock_data, 'Original_Open', window_size)\n",
    "stock_data['High_normalized'] = rolling_window_normalization(stock_data, 'Original_High', window_size)\n",
    "stock_data['Low_normalized'] = rolling_window_normalization(stock_data, 'Original_Low', window_size)\n",
    "\n",
    "stock_data['Original_Taker_buy__base_asset_volume_normalized'] = rolling_window_normalization(stock_data, 'Original_Taker_buy__base_asset_volume', window_size)\n",
    "stock_data['Original_Taker_buy__quote_asset_volume_normalized'] = rolling_window_normalization(stock_data, 'Original_Taker_buy__quote_asset_volume', window_size)\n",
    "\n",
    "stock_data['Volume_normalized'] = rolling_window_normalization(stock_data, 'Volume', window_size)\n",
    "stock_data['Number_of_trades_normalized'] = rolling_window_normalization(stock_data, 'Original_Number_of_trades', window_size) \n",
    "\n",
    "# stock_data['Close_normalized'] = min_max_normalization(stock_data, 'Original_Close')\n",
    "# stock_data['Volume_normalized'] = min_max_normalization(stock_data, 'Volume')\n",
    "\n",
    "# Min-Max normalisering\n",
    "stock_data['EMA50_normalized'] = min_max_normalization(stock_data, 'EMA50')\n",
    "stock_data['EMA100_normalized'] = min_max_normalization(stock_data, 'EMA100')\n",
    "stock_data['MACD_normalized'] = min_max_normalization(stock_data, 'MACD')\n",
    "\n",
    "# Fjern NaN-verdier som kan oppstå etter rullende vindu normalisering\n",
    "stock_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "stockDataHandler.CleanData(stock_data)\n",
    "\n",
    "def generate_target(df, column_name, steps_ahead=1):\n",
    "    \"\"\"\n",
    "    Genererer en 'Target'-kolonne basert på fremtidig pris.\n",
    "    Prisen 'steps_ahead' punkter frem i tid vil være målverdien.\n",
    "    \"\"\"\n",
    "    df['Target'] = df[column_name].shift(-steps_ahead)\n",
    "    df.dropna(inplace=True)  # Fjerner NaN-verdier som kan oppstå på grunn av tidsforskyvningen\n",
    "    return df\n",
    "\n",
    "# Bruk funksjonen for å generere 'Target'-kolonnen basert på 'Original_Close'-kolonnen\n",
    "stock_data = generate_target(stock_data, 'Close_normalized',1)\n",
    "\n",
    "def create_sequences(X, y, sequence_length):  \n",
    "    X_sequences = []  \n",
    "    y_sequences = []  \n",
    "  \n",
    "    for i in range(len(X) - sequence_length):  \n",
    "        X_sequences.append(X[i:i + sequence_length])  \n",
    "        y_sequences.append(y[i + sequence_length])  \n",
    "  \n",
    "    X_sequences = np.array(X_sequences)  \n",
    "    y_sequences = np.array(y_sequences)  \n",
    "  \n",
    "    return X_sequences, y_sequences  \n",
    "\n",
    "# Prepare data for sequences  \n",
    "X = stock_data[['Close_normalized','Open_normalized','High_normalized', 'Low_normalized', 'Number_of_trades_normalized', 'Volume_normalized','Original_Taker_buy__base_asset_volume_normalized','Original_Taker_buy__quote_asset_volume_normalized', 'EMA50_normalized', 'EMA100_normalized', 'MACD_normalized']]  \n",
    "y = stock_data['Target']  \n",
    "  \n",
    "sequence_length = 1  # Antall tidssteg du ønsker å bruke som input  \n",
    "  \n",
    "# Create sequences before reshaping and splitting  \n",
    "X_sequences, y_sequences = stockDataHandler.create_sequences(X.values, y.values, sequence_length)  \n",
    "  \n",
    "# Forme data for LSTM (samples, timesteps, features)  \n",
    "X_sequences = np.reshape(X_sequences, (X_sequences.shape[0], sequence_length, X_sequences.shape[2]))  \n",
    "  \n",
    "# Splitte data i trening og testsett  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.3, random_state=42)  \n",
    "\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(500, return_sequences=True, input_shape=(X_train.shape[1], 11))) \n",
    "model.add(layers.LSTM(500, return_sequences=False))\n",
    "model.add(layers.Dense(50))\n",
    "model.add(layers.Dense(1))\n",
    "# model.add(layers.Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "def custom_root_mean_squared_error(y_true, y_pred, percent_threshold=1.0):\n",
    "    # Beregn prosentavviket mellom y_true og y_pred\n",
    "    percent_error = 100 * np.abs((y_pred - y_true) / y_true)\n",
    "    \n",
    "    # Beregn RMSE bare for de eksemplene der prosentavviket er innenfor det angitte prosentintervallet\n",
    "    squared_errors = np.square(y_pred - y_true)\n",
    "    filtered_squared_errors = np.where(percent_error <= percent_threshold, squared_errors, 0)\n",
    "    \n",
    "    # Beregn RMSE for de filtrerte feilene\n",
    "    rmse = np.sqrt(np.mean(filtered_squared_errors))\n",
    "    return rmse\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])  # endre til loss='categorical_crossentropy' for klassifisering\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[root_mean_squared_error])  # endre til loss='categorical_crossentropy' for klassifisering\n",
    "\n",
    "\n",
    "# Trene modellen\n",
    "#model.fit(X_train, y_train, epochs=1000, batch_size=8)\n",
    "\n",
    "# Definer antall epoker for lagring av modellen  \n",
    "save_every_n_epochs = 1  # Endre dette tallet etter ønske  \n",
    "  \n",
    "# Opprett en katalog for å lagre modellene  \n",
    "models_directory = 'saved_models_train_v4'  \n",
    "if not os.path.exists(models_directory):  \n",
    "    os.makedirs(models_directory)  \n",
    "  \n",
    "# Definer filnavnformatet for å reflektere antall epoker modellen er trent på  \n",
    "filename_format = os.path.join(models_directory, 'lstm_model_2017-06_2023_train_v4_epoch-{epoch:03d}_500_500_50_1.h5')  \n",
    "  \n",
    "r2_callback = R2ScoreCallback(X_test, y_test, save_every_n_epochs)\n",
    "\n",
    "# Legge til både checkpoint og R2ScoreCallback i callbacks-listen\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=15, callbacks=[checkpoint, r2_callback])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
